---
layout:     post
title:     2020-08-29-ROBUST-REINFORCEMENT-LEARNING FOR-CONTINUOUS-CONTROL-WITH-MODEL-MISSPECIFICATION
subtitle:   paper
date:       2020-08-29
author:     wucy
header-img: img/yu.jpg
catalog: true
tags:
    - reinforcementlearning paper
---


# <center>论文笔记</center>

# 标题

具有模型误差的连续控制鲁棒强化学习

# 摘要

我们提供了一个框架来将鲁棒性纳入，来减少因模型误差带来的扰动，在持续的强化学习中。我们明确致力于将鲁棒性纳入最先进的连续控制RL算法中称为MPO。我们通过学习一个优化最差情况，或者导出相应的鲁棒熵正则贝尔曼算子，来达到这个目的。另外，我们介绍一个保守的，软鲁棒鲁棒熵正则贝尔曼算子。我们发现鲁棒和软鲁棒策略在九个mujoco环境中都跑赢了无鲁棒性。另外，我们展示了鲁棒性在高维虚拟灵巧的机器手臂中的改进。最后，我们提出多个调查性实验，提供对稳健性更深入的见解框架。这包括一个对其他持续性强化学习策略和离线数据学习不确定性的适应。

# MPO实验结论

我们提供一个框架来将鲁棒性纳入连续控制强化学习来减少动态过渡里的误差。这个框架适合于持续性控制策略，比如演员评论家设置。我们将鲁棒性纳入MPO和交叉熵正则化的MPO版本。另外，我们展示一个将鲁棒性纳入随机梯度算法的实验。从一个理论的角度，我们将MPO改编为Entropyregulated版本（E-MPO），我们之后合并鲁棒性进 R-MPO RE-MPO SR-MPO/SRE-MPO。这是通过推导相应的鲁棒和软鲁棒熵正则Bellman算子来实现的，以确保策略评估步骤在每种情况下都收敛。我们有扩展的实验来展示有鲁班的版本超过了没有鲁棒的版本，在大多数情况下，这叫藏一手。我还提供各种各样的协商实验来理解鲁棒和软鲁棒策略的更多细节。包括实验展示优化鲁棒在R-MPO上的表现，当使用一个不确定性的传输模型从离轨的数据学习策略。

# 背景

**MDP**
$$
V^{\pi}(s)=E^{\pi}[\sum_{t=0}^{\infin}\gamma^tr_t|s_0=s]\\
Q^{\pi}(s,a)=r(s,a)+\gamma E_{s'\sim P(|s,a)}[V^{\pi}(s')]
$$


**R-MDP**
$$
V_{R}^{\pi}(s)=inf_{p\in P}E^{p,\pi}[\sum_{t=0}^{\infin}\gamma^{t}r_{t}|s_{0}=s]\\
Q_R^{\pi}(s) = r(s,a) + \gamma inf_{p\in P}E_{s'\sim p(.|s,a)}[V_R^{\pi}(s')]
$$
**MPO**

评价评价和策略改进，
$$
价值评价的目的\min_{\theta}(r_{t}+\gamma Q _{\tilde{\theta}}^{\pi_{k}}(S_{t+1}\sim P(.|s_t,a_t),a_{t+1}\sim\pi_{k}(.|s_{t+1}))-Q_{\theta}^{\pi_{k}}(s_t,a_t)^2)
$$

$$
\begin{align}
&价值改进策略，优化\overline{J}(s,\pi)=E_{\pi}[Q^{\pi_k}_{\theta}(s,a)]\\&
首先构建一个非参数的估计值q，如\overline{J}(s,q) \ge \overline{J}(s,\pi_{k}),最大化\overline J(s,q)接近取当前策略\pi_{k}\\
&E_{\mu(s)}[KL(q(.|s),\pi_{k}(.|s))] <\epsilon\\
&此优化有一个封闭形式的解决方案
\end{align}
$$
**ROBUST MPO**

为了把鲁棒性纳入MPO，我们致力于在评价步骤中学习一个最差的价值函数。这样评价步骤可以被纳入AC算法，特别是优化TD误差，我们定义它为
$$
\min_{\theta}(r_{t}+\gamma Q _{\tilde{\theta}}^{\pi_{k}}(S_{t+1}\sim P(.|s_t,a_t),a_{t+1}\sim\pi_{k}(.|s_{t+1}))-Q_{\theta}^{\pi_{k}}(s_t,a_t)^2)
$$


# 实验

实验分为三个部分。第一部分详细介绍了鲁棒和软鲁棒训练的设置。下一节将每个域中的鲁棒和软鲁棒性能与非鲁棒MPO基线进行比较。最后一部分是一组调查性实验，以获得对鲁棒性和软性鲁棒性代理性能的额外见解。

对于每一个领域，鲁棒agent被训练成一个预先定义的由三个任务扰动组成的不确定性集。三个扰动项都对应一个特定的扰动。

例如，在Cartpole中，不确定性集由三个不同的杆长度。在三个未知任务扰动的测试集上对鲁棒代理和非鲁棒代理进行了评估。注意，通常的做法是手动选择预定义的不确定度集和未查看的测试环境。

# 鲁棒熵正则BELLMAN算子

为了在开发鲁棒RL算法时利用这一思想，我们将鲁棒Bellman算子推广到鲁棒熵正则Bellman算子，并证明了它是一个收缩算子。

鲁棒熵正则定义为
$$
V_{R-KL}^{\pi}(s;\overline{\pi})=E_{a\sim\pi(.|s)}[r(s,a)-\tau \log \frac{\pi(.|s)}{\overline \pi(.|s)}+\gamma inf_{p\in P}E_{s'\sim p(.|s,a)}[V(s')]]
$$
 # 调查性实验 

**如果我们增加训练样本的数量呢？**

我们对非鲁棒的实验多进行了三次训练，效果反而更差

**域随机化？**

**DR和RE-MPO/SRE-MPO之间的直观区别是什么？**

TD定义了误差的不确定性损失。每个TD误差都是用一个状态，动作，奖赏，下一个状态<s，a，r，s0>轨迹来计算的，（从不确定性集中统一选择）。

在鲁棒性的情况下，计算TD误差，使得目标动作值函数被计算为相对于不确定集的最坏情况值函数，这意味着所学的策略在训练期间显式地搜索对抗性示例，以说明最坏情况下的性能。

在软鲁棒情况下，DR的细微但重要的区别（如实验所示）是，TD损耗是用相对于下一个状态的平均目标作用值函数计算的（而不是像DR那样平均每个单独扰动环境的TD误差）。这将导致与DR相比，使用不同的梯度更新来更新动作值函数。

**把鲁棒性和其他算法结合起来怎么样 **

为了证明这种鲁棒性方法的推广性，我们将其应用到随机值梯度（SVG）连续控制RL算法中。

**稳健熵正则化收益与稳健预期收益**

当比较稳健熵正则化收益率与稳健预期收益率时，我们发现熵正则化收益率似乎并不比预期收益率差

**从离线数据中学习不确定性集怎么样**

在现实世界的环境中，例如机器人和工业控制中心（Gao，2014），可能有一个名义上可用的模拟器，以及从真实世界系统捕获的离线数据。这些数据可以用来训练过渡模型，以捕捉手头任务的动态。

然而，我们是否可以从这些数据中训练出一组过渡模型，将它们作为R-MPO中的不确定性集，并且在标称模拟器上进行训练时仍能产生鲁棒性能？

对于摆锤摆动，我们改变了杆的质量并生成了相应的数据集。然后，我们在越来越大的数据批处理范围内训练转换模型。

（1）正如预期的那样，对于小批量数据，模型过于不准确，导致性能较差

（2）一个有趣的发现是，随着数据批处理大小的增加，DDR-MPO的性能开始优于R-MPO，特别是对于越来越大的扰动

（3）随着批处理规模的进一步增大，过渡模型越来越接近真实模型，DDR-MPO收敛到R-MPO的性能