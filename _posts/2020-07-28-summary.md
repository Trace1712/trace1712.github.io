---
layout:     post
title:      总结
subtitle:   summary
date:       2020-07-28
author:     wucy
header-img: img/yu.jpg
catalog: true
tags:
    - reinforcementlearning
---


# <center>第五章总结</center>

### 知识点及疑问

* 首次访问型MC算法：用s的所有首次访问的回报的平均值估计v<sub>&pi;</sub>（s)。
* 同轨策略方法中，策略一般是“软性”的，即对于任意s&in;S以及a&in;A(s)，都有&pi;(a|s)>0，但它们会逐渐逼近一个确定性的策略。

* 同轨策略其实是一种妥协——它并不学习最优策略，而是学习一个接近最哟而且可以进行探索的策略的动作值。

* 一个更加直接的方法是干脆采用两个策略，一个用来学习并最终成为最优策略，另一个更加具有试探性，用来产生智能体得行动样本。用来学习的策略称为**目标策略**，用于获取行动样本的称为**行动策略**。这种过程叫离轨策略。

* <font color="red">离轨策略中的两个策略有什么关系？或者说学到的最优策略对探索有什么好处。</font>

* 几乎所有的离轨策略都使用**重要性采样**，我们只需要根据重要度采样比来调整回报值并对结果进行平均即可。另一种方法是**加权重要度采样**。
  $$
  \begin{align}
  重要性采样 &V(s)=\frac{\sum_{t\in\tau(s)}\rho_{t:T(t)-1}G_t}{|\tau(s)|}\\
  加权重要度采样&V(s)=\frac{\sum_{t\in\tau(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\tau(s)}\rho_{t:T(t)-1}}
  \end{align}
  $$

### 不会

**Exercise5.5**

考虑只有一个非终止状态和一个单一动作的马尔科夫决策过程，动作设定为：以概率p跳回非终止状态，以概率1-p转移到终止状态。令每一时刻所有转移的收益都是+1，且&gamma;=1假设观察到一幕序列持续10个时刻，获得了值为10的回报。则对这个非终止状态的首次访问型和每次访问型的价值估计分别是什么？

**Exercise 5.13**

给出公式（5.12）推导公式（5.14）的详细步骤

**Exercise5.14**

使用截断加权平均估计器（式5.10）的思想修改离轨策略的蒙特卡洛控制算法（见5.7节）。